{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdditionalNLP.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRRojUXEOd15yHNNrywyL1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words / Word2Vec\n",
        "\n",
        "Most fundamental representation of text. Bag of words considers the frequency of words relative to size of text (text frequency). With each word having a probability to then compare different kinds of text based on frequency. E.g. spam contains higher frequency of certain words than non-spam so an algorithm can be applied on this data.\n",
        "\n",
        "Word2Vec accounts for position of words to create a vector representation of that word."
      ],
      "metadata": {
        "id": "FcYujZeL7eW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigrams / Trigrams - TFIDF Decomposition to find Topics\n",
        "\n",
        "Bigrams and Trigrams are two or three words frequently occurring together in a document. Some examples in my dataset include: ‘block based programming’, ‘visually impaired’, ‘screen reader’, ‘programming language’, ‘computer science’, etc. I used Gensim’s Phrases model to build and implement the bigrams and trigrams. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams or trigrams."
      ],
      "metadata": {
        "id": "xZKQHI_f8HNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(tokenized_data, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[tokenized_data], threshold=10)"
      ],
      "metadata": {
        "id": "VB7b5qwL8JOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "qQU287vY8KsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions for creating bigrams and trigrams.\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "# Form Bigrams\n",
        "tokenized_data_bigrams = make_bigrams(tokenized_data)\n",
        "# Form Trigrams\n",
        "tokenized_data_trigrams = make_trigrams(tokenized_data)"
      ],
      "metadata": {
        "id": "Qz-WTU498L3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# de-tokenization, combine tokens together\n",
        "detokenized_data = []\n",
        "for i in range(len(dataset)):\n",
        "    t = ' '.join(tokenized_data_trigrams[i])\n",
        "    detokenized_data.append(t)\n",
        "dataset['clean_text']= detokenized_data\n",
        "documents = dataset['clean_text']"
      ],
      "metadata": {
        "id": "XaFW3Is38N1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first step towards topic modelling. Each and every term and document in the dataset has to be represented as a vector. We will use sklearn’s TfidfVectorizer to create a document-term matrix using only 1000 terms (words) from our corpus. "
      ],
      "metadata": {
        "id": "EtwiIJ698ZHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set variable number of terms\n",
        "no_terms = 1000\n",
        "# NMF uses the tf-idf count vectorizer\n",
        "# Initialise the count vectorizer with the English stop words\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=no_terms, stop_words='english')\n",
        "# Fit and transform the text\n",
        "document_matrix = vectorizer.fit_transform(documents)\n",
        "#get features\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "metadata": {
        "id": "wn4GPObb8apA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generated document-term matrix will be decomposed into multiple matrices. We will use sklearn’s decomposition model NMF to perform the task of matrix decomposition. The number of topics to be generated can be specified by using the n_components parameter. The resulting matrices derived after running the topic model are the document-topic matrix and term-topic matrix. In the term-topic matrix, sorting the rows in reverse, reveals the top terms for each topic."
      ],
      "metadata": {
        "id": "AQp_mBJ78ds3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set variables number of topics and top words.\n",
        "no_topics = 10\n",
        "no_top_words = 10\n",
        "# Function for displaying topics\n",
        "def display_topic(model, feature_names, num_topics, no_top_words, model_name):\n",
        "    print(\"Model Result:\")\n",
        "    word_dict = {}\n",
        "    for i in range(num_topics):\n",
        "      #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "       words_ids = model.components_[i].argsort()[:-no_top_words - 1:-1]\n",
        "       words = [feature_names[key] for key in words_ids]\n",
        "       word_dict['Topic # ' + '{:02d}'.format(i)] = words\n",
        "    dict = pd.DataFrame(word_dict)\n",
        "    dict.to_csv('%s.csv' % model_name)\n",
        "    return dict\n",
        "# Apply NMF topic model to document-term matrix\n",
        "nmf_model = NMF(n_components=no_topics, random_state=42, alpha=.1, l1_ratio=.5, init='nndsvd').fit(document_matrix)"
      ],
      "metadata": {
        "id": "OCHw2pEQ8bC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-means Clustering of Text - Summarisation\n",
        "\n",
        "Word2Vec vector representation of words and combined for sentences. Run K-means algorithm to find centers (which could be interpreted as topics) and select sentence vectors closest to the centers as the summarisation."
      ],
      "metadata": {
        "id": "l0G2mhd29CO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')   # one time execution\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "1leciI5L9Gjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "nltk.download('stopwords')  # one time execution\n",
        "from nltk.corpus import stopwords\n",
        "corpus = []\n",
        "for i in range(len(sentence)):\n",
        "    sen = re.sub('[^a-zA-Z]', \" \", sentence[i])  \n",
        "    sen = sen.lower()                            \n",
        "    sen = sen.split()                         \n",
        "    sen = ' '.join([i for i in sen if i not in stopwords.words('english')])   \n",
        "    corpus.append(sen)"
      ],
      "metadata": {
        "id": "bPNZy-Ag9G_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "all_words = [i.split() for i in corpus]\n",
        "model = Word2Vec(all_words, min_count=1,size= 300)"
      ],
      "metadata": {
        "id": "1FaaRXgL9Ixm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_vector=[]\n",
        "for i in corpus:\n",
        "    plus=0\n",
        "    for j in i.split():\n",
        "        plus+= model.wv[j]\n",
        "    plus = plus/len(i.split())\n",
        "    sent_vector.append(plus)"
      ],
      "metadata": {
        "id": "YVEM4XLY9P3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters, init = 'k-means++', random_state = 42)\n",
        "y_kmeans = kmeans.fit_predict(sent_vector)"
      ],
      "metadata": {
        "id": "KfF7yqM-9S1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentence vector which has the minimum euclidean distance from the cluster centroid represents the whole group. These sentences from each cluster are ordered in the similar fashion as the original text to form a meaningful summary."
      ],
      "metadata": {
        "id": "K1kyTx739XYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "my_list=[]\n",
        "for i in range(n_clusters):\n",
        "    my_dict={}\n",
        "    \n",
        "    for j in range(len(y_kmeans)):\n",
        "        \n",
        "        if y_kmeans[j]==i:\n",
        "            my_dict[j] =  distance.euclidean(kmeans.cluster_centers_[i],sent_vector[j])\n",
        "    min_distance = min(my_dict.values())\n",
        "    my_list.append(min(my_dict, key=my_dict.get))\n",
        " \n",
        "                            \n",
        "for i in sorted(my_list):\n",
        "    print(sentence[i])"
      ],
      "metadata": {
        "id": "xRq6D89a9X1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modelling with LDA\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is part of Python's Gensim package.\n",
        "LDA was first developed by Blei et al. in 2003. LDA is a generative probabilistic model similar to Naive Bayes. It represents topics as word probabilities and allows for uncovering latent or hidden topics as it clusters the words based on their co-occurrence in a respective document."
      ],
      "metadata": {
        "id": "a5g5B7rS3QQ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yw3JbVM2Yjj"
      },
      "outputs": [],
      "source": [
        "# Install\n",
        "!pip install pyLDAvis -qq\n",
        "!pip install -qq -U gensim\n",
        "!pip install spacy -qq\n",
        "!pip install matplotlib -qq\n",
        "!pip install seaborn -qq\n",
        "!python -m spacy download en_core_web_md -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import spacy\n",
        "import pyLDAvis.gensim_models\n",
        "pyLDAvis.enable_notebook()# Visualise inside a notebook\n",
        "import en_core_web_md\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import CoherenceModel"
      ],
      "metadata": {
        "id": "LUiO7vKV2c18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our spaCy model:\n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "# Tags I want to remove from the text\n",
        "removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
        "tokens = []"
      ],
      "metadata": {
        "id": "S4tcIDll2eUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for summary in nlp.pipe(reports['summary']):\n",
        "   proj_tok = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
        "   tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "N6jQFFib2f9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add tokens to new column\n",
        "reports['tokens'] = tokens\n",
        "reports['tokens']"
      ],
      "metadata": {
        "id": "qJSzE8b72g3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary\n",
        "# I will apply the Dictionary Object from Gensim, which maps each word to their unique ID:\n",
        "dictionary = Dictionary(reports['tokens'])\n",
        "print(dictionary.token2id)\n",
        "\n",
        "# Filter dictionary\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
        "\n",
        "# Create corpus\n",
        "corpus = [dictionary.doc2bow(doc) for doc in reports['tokens']]"
      ],
      "metadata": {
        "id": "uDuMNm592hx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model building\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, num_topics=10, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "G0KHUU8c2lXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having constructed the topics, a coherence score can be computed. The score measures the degree of semantic similarity between high scoring words in each topic. In this fashion, a coherence score can be computed for each iteration by inserting a varying number of topics.\n",
        "\n",
        "A range of algorithms has been introduced to calculate the coherence score (C_v, C_p, C_uci, C_umass, C_npmi, C_a, …). Working with the gensim library makes computing these coherence measures for topic models fairly simple. I personally choose to implement C_v and C_umass. The coherence score for C_v ranges from 0 (complete incoherence) to 1 (complete coherence). Values above 0.5 are fairly good, according to John McLevey"
      ],
      "metadata": {
        "id": "VqCkAbD423ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Coherence score using C_umass:\n",
        "topics = []\n",
        "score = []\n",
        "for i in range(1,20,1):\n",
        "   lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n",
        "   cm = CoherenceModel(model=lda_model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
        "   topics.append(i)\n",
        "   score.append(cm.get_coherence())\n",
        "_=plt.plot(topics, score)\n",
        "_=plt.xlabel('Number of Topics')\n",
        "_=plt.ylabel('Coherence Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OyiS0lE52mqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coherence score using C_v:\n",
        "topics = []\n",
        "score = []\n",
        "for i in range(1,20,1):\n",
        "   lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n",
        "   cm = CoherenceModel(model=lda_model, texts = reports['tokens'], corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
        "   topics.append(i)\n",
        "   score.append(cm.get_coherence())\n",
        "_=plt.plot(topics, score)\n",
        "_=plt.xlabel('Number of Topics')\n",
        "_=plt.ylabel('Coherence Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uPA_-4hm2oVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When looking at the coherence using the C_umass or C_v algorithm, the best is usually the max. Looking at the graphs I choose to go with 5 topics, although no certain answer can be given."
      ],
      "metadata": {
        "id": "Eb2pa8Wb3EW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal model\n",
        "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=100, num_topics=5, workers = 4, passes=100)\n",
        "\n",
        "# Print topics\n",
        "lda_model.print_topics(-1)\n",
        "\n",
        "# Where does a text belong to\n",
        "lda_model[corpus][0]\n",
        "reports['summary'][0]\n",
        "\n",
        "# Visualize topics\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(lda_display)\n",
        "\n",
        "# Save the report\n",
        "pyLDAvis.save_html(lda_display, 'index.html')"
      ],
      "metadata": {
        "id": "WsKcyLQk2qSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained Sentiment Classifiers\n",
        "\n",
        "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically designed to detect sentiments expressed in social media.\n",
        "\n",
        "pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all add up to be 1 or close to it with float operation).\n",
        "\n",
        "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and 1 (most extreme positive). This is the most useful metric if you want a single measure of sentiment for a given sentence."
      ],
      "metadata": {
        "id": "2_jiDXlY3cWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install vaderSentimentlibrary\n",
        "pip install vaderSentiment\n",
        "\n",
        "#import the library\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "_n3cQp7b2rds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the negative, positive, neutral and compound scores, plus verbal evaluation\n",
        "def sentiment_vader(sentence):\n",
        "\n",
        "    # Create a SentimentIntensityAnalyzer object.\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        "\n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "    negative = sentiment_dict['neg']\n",
        "    neutral = sentiment_dict['neu']\n",
        "    positive = sentiment_dict['pos']\n",
        "    compound = sentiment_dict['compound']\n",
        "\n",
        "    if sentiment_dict['compound'] >= 0.05 :\n",
        "        overall_sentiment = \"Positive\"\n",
        "\n",
        "    elif sentiment_dict['compound'] <= - 0.05 :\n",
        "        overall_sentiment = \"Negative\"\n",
        "\n",
        "    else :\n",
        "        overall_sentiment = \"Neutral\"\n",
        "  \n",
        "    return negative, neutral, positive, compound, overall_sentiment"
      ],
      "metadata": {
        "id": "5y486z9z3rK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}